[
  {
    "objectID": "posts/2023-03-13-spotify-ML-blog/index.html",
    "href": "posts/2023-03-13-spotify-ML-blog/index.html",
    "title": "Whose Song is it Anyway?",
    "section": "",
    "text": "Have you ever wondered how your music taste compares to your friends’? Or perhaps you’re just curious about how your favorite songs and artists and genres stack up against your peers. As a music lover, I enjoy discussing these topics and was thrilled when an assignment for my machine learning course involved analyzing Spotify data. For the assignment, my friend Elke and I shared our library of liked songs on Spotify so we could visualize comparisons between our music tastes and then create a model to try and predict whose library a song is in. \nThis blog post will have the following sections:\n\nVisualizing Spotify Data\nStatistical Tests: Prelude to Formal Classification\nSummarizing the classification models \nComparing model performance \nFinal thoughts \n\nNow, let’s dive into the fascinating world of music data analysis and discover whose song it is anyway!"
  },
  {
    "objectID": "posts/2023-03-13-spotify-ML-blog/index.html#visualizing-spotify-data",
    "href": "posts/2023-03-13-spotify-ML-blog/index.html#visualizing-spotify-data",
    "title": "Whose Song is it Anyway?",
    "section": "Visualizing Spotify Data",
    "text": "Visualizing Spotify Data\nBefore getting started with the modeling, I wanted to explore the patterns of our data visually. Looking at the variables I was able to pull in from the API, I found the following particularly interesting:\n\nAccousticness (0–1):  Whether the track primarily uses instruments that produce sound through acoustic means as opposed to electric or electronic means.\nDanceability (0–1): The ease with which someone can dance to a song based on a combination of musical elements like tempo, rhythm stability, beat strength, and overall regularity.\nEnergy (0–1): Represents a perceptual measure of intensity and activity. Song features that determine the energy score include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\nSpeechiness (0–1): Speechiness detects the presence of spoken words in a track.\nValence (0–1): Describes whether a song’s vibe is positive or negative—tracks with high valence sound more positive (e.g. happy/cheerful), while tracks with low valence sound more negative (e.g. sad/angry).\n\nTo explore which songs in my library have particularly high/low scores for the above metrics, I created an interactive table (and learned that my taste in music is fairly chaotic!).\n\n\nCode\nlibrary(DT)\n\nlewis_liked_tracks %>%\n  dplyr::select(track.name, primary_artist, danceability, acousticness, speechiness, energy, valence) %>%\n  datatable()\n\n\n\n\n\n\n\nAfter exploring the songs in my library, I transitioned my focus to comparing my music to Elke’s.\n\n\nCode\n#Danceability comparison\ndance_plot <- ggplot(all_tracks, aes(x = danceability, fill = listener,\n                    text = paste(listener))) +\n  geom_density(alpha=0.6, color=NA) +\n  scale_fill_manual(values=c(\"#b0484f\", \"#4436d9\"))+\n  labs(x=\"Danceability\", y=\"Density\") +\n  guides(fill=guide_legend(title=\"Listener\"))+\n  theme_minimal() +\n  ggtitle(\"Distribution of Danceability Data\")\n\n\n#speechiness comparison\nspeech_plot <- ggplot(all_tracks, aes(x = speechiness, fill = listener,\n                    text = paste(listener))) +\n  geom_density(alpha=0.6, color=NA) +\n  scale_fill_manual(values=c(\"#b0484f\", \"#4436d9\"))+\n  labs(x=\"Speechiness\", y=\"Density\") +\n  guides(fill=guide_legend(title=\"Listener\"))+\n  theme_minimal() +\n  ggtitle(\"Distribution of Speechiness Data\")\n\n\n#acousticness comparison\nacoustic_plot <- ggplot(all_tracks, aes(x = acousticness, fill = listener,\n                    text = paste(listener))) +\n  geom_density(alpha=0.6, color=NA) +\n  scale_fill_manual(values=c(\"#b0484f\", \"#4436d9\"))+\n  labs(x=\"Acousticness\", y=\"Density\") +\n  guides(fill=guide_legend(title=\"Listener\"))+\n  theme_minimal() +\n  ggtitle(\"Distribution of Acousticness Data\")\n\n#energy comparison\nenergy_plot <- ggplot(all_tracks, aes(x = energy, fill = listener,\n                    text = paste(listener))) +\n  geom_density(alpha=0.6, color=NA) +\n  scale_fill_manual(values=c(\"#b0484f\", \"#4436d9\"))+\n  labs(x=\"Energy\", y=\"Density\") +\n  guides(fill=guide_legend(title=\"Listener\"))+\n  theme_minimal() +\n  ggtitle(\"Distribution of Energy Data\")\n\nggarrange(dance_plot, speech_plot, acoustic_plot, energy_plot, ncol=2, nrow=2, common.legend = TRUE, legend=\"bottom\")\n\n\n\n\n\nA few main takeaways:\n\nBoth Elke and I tend to listen to fairly danceable music, but my library includes more songs that are rated as highly danceable. \nMy library scored much higher in “speechiness” overall — I primarily listen to a mixture of rap and “sad girl” music, so I wasn’t surprised that my library contained more songs with spoken words than Elke’s. \nThe energy in our songs is quite similar, but my library scored a little higher here.\nWhile both of our libraries spanned the full range of acousticness, Elke’s library had more highly acoustic songs while I had more that score very low on this metric. \n\n\n\nCode\n#plotting valence and energy to get a sense for the moods of our liked songs\nggplot(data = all_tracks, aes(x = valence, y = energy, color = listener)) +\n  geom_point(alpha = 0.5) +\n  geom_vline(xintercept = 0.5) +\n  geom_hline(yintercept = 0.5) +\n  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +\n  annotate('text', 0.25 / 2, 0.95, label = \"Turbulent/Angry\", fontface =\n             \"bold\") +\n  annotate('text', 1.75 / 2, 0.95, label = \"Happy/Joyful\", fontface = \"bold\") +\n  annotate('text', 1.75 / 2, 0.05, label = \"Chill/Peaceful\", fontface =\n             \"bold\") +\n  annotate('text', 0.25 / 2, 0.05, label = \"Sad/Depressing\", fontface =\n             \"bold\") +\n  theme_minimal() +\n  labs(x = \"Valence\",\n       y = \"Energy\",\n       title = \"Plotting songs based on their positivity and energy level\",\n       subtitle = \"Elke and I don't have many songs in the Chill/Peaceful quadrant.\")\n\n\n\n\n\nThis above visualization compares the energy and valence of songs. To interpret the plot, it’s important to think about how energy and valence interact. \n\nLow Energy + Low Valence =  Sad / Depressing\nLow Energy + High Valence = Chill / Peaceful\nHigh Energy + High Valence = Happy / Upbeat\nHigh Energy + Low Valence = Turbulent / Angry  \n\nBased on the plot, it looks like neither Elke nor I listen to much chill/peaceful music (which I found somewhat ironic as my most popular playlist on spotify is called “chill”). Aside from this, our music seems to span the graph fairly evenly, although we both listen to music that tends to be a little higher energy. \nTo see which songs represent the various points on this graph, check out an interactive version of the plot below."
  },
  {
    "objectID": "posts/2023-03-13-spotify-ML-blog/index.html#statistical-tests-prelude-to-formal-classification",
    "href": "posts/2023-03-13-spotify-ML-blog/index.html#statistical-tests-prelude-to-formal-classification",
    "title": "Whose Song is it Anyway?",
    "section": "Statistical Tests: Prelude to Formal Classification",
    "text": "Statistical Tests: Prelude to Formal Classification\nTo get an initial sense for whether my classification algorithms might be successful, I decided to run a few t-tests on my variables of interest to see whether my library is different from Elke’s on a statistical level. As can be seen from the tables below, all results were significant at the 0.001 alpha level, suggesting that prediction should be at least somewhat successful.\n\n\nCode\nlibrary(sjPlot)\n\ndance_test_table <- tab_model(t.test(lewis_liked_tracks$danceability, elke_liked_tracks$danceability, var.equal = FALSE),\n          string.ci = c(\"Conf. Int (95%)\"),\n          string.p = \"P-value\",\n          dv.labels = c(\"Danceability Score\"),\n          pred.labels = \"Lewis – Elke\",\n          title = \"Table 1: Comparing Danceability Scores: Welch Two Sample t-test\")\n\ndance_test_table\n\n\n\n\nTable 1: Comparing Danceability Scores: Welch Two Sample t-test\n\n \nDanceability Score\n\n\nestimate\nConf. Int (95%)\nP-value\nPredictors\n\n\n0.09\n0.07 – 0.10\n<0.001\nLewis – Elke\n\n\n\n\n\n\nCode\nspeech_test_table <- tab_model(t.test(lewis_liked_tracks$speechiness, elke_liked_tracks$speechiness, var.equal = FALSE),\n          string.ci = c(\"Conf. Int (95%)\"),\n          string.p = \"P-value\",\n          dv.labels = c(\"Speechiness Score\"),\n          pred.labels = \"Lewis – Elke\",\n          title = \"Table 2: Comparing Speechiness Scores: Welch Two Sample t-test\")\n\nspeech_test_table\n\n\n\n\nTable 2: Comparing Speechiness Scores: Welch Two Sample t-test\n\n \nSpeechiness Score\n\n\nestimate\nConf. Int (95%)\nP-value\nPredictors\n\n\n0.10\n0.09 – 0.11\n<0.001\nLewis – Elke\n\n\n\n\n\n\nCode\nacoustic_test_table <- tab_model(t.test(lewis_liked_tracks$acousticness, elke_liked_tracks$acousticness, var.equal = FALSE),\n          string.ci = c(\"Conf. Int (95%)\"),\n          string.p = \"P-value\",\n          dv.labels = c(\"Acousticness Score\"),\n          pred.labels = \"Lewis – Elke\",\n          title = \"Table 2: Comparing Acousticness Scores: Welch Two Sample t-test\")\n\nacoustic_test_table\n\n\n\n\nTable 2: Comparing Acousticness Scores: Welch Two Sample t-test\n\n \nAcousticness Score\n\n\nestimate\nConf. Int (95%)\nP-value\nPredictors\n\n\n-0.13\n-0.16 – -0.09\n<0.001\nLewis – Elke\n\n\n\n\n\n\nCode\nenergy_test_table <- tab_model(t.test(lewis_liked_tracks$energy, elke_liked_tracks$energy, var.equal = FALSE),\n          string.ci = c(\"Conf. Int (95%)\"),\n          string.p = \"P-value\",\n          dv.labels = c(\"Energy Score\"),\n          pred.labels = \"Lewis – Elke\",\n          title = \"Table 2: Comparing Energy Scores: Welch Two Sample t-test\")\n\nenergy_test_table\n\n\n\n\nTable 2: Comparing Energy Scores: Welch Two Sample t-test\n\n \nEnergy Score\n\n\nestimate\nConf. Int (95%)\nP-value\nPredictors\n\n\n0.05\n0.03 – 0.07\n<0.001\nLewis – Elke"
  },
  {
    "objectID": "posts/2023-03-13-spotify-ML-blog/index.html#summarizing-the-classification-models",
    "href": "posts/2023-03-13-spotify-ML-blog/index.html#summarizing-the-classification-models",
    "title": "Whose Song is it Anyway?",
    "section": "Summarizing the classification models",
    "text": "Summarizing the classification models\nFor this assignment, we were asked to try a number of different classification methods and compare their results. Here, I used the following methods:\nK-Nearest Neighbors (KNN)\n\nEach observation is predicted based on its ‘similarity’ to other observations. In summary, the algorithm identifies K observations that are “similar” (by some distance metric, often Euclidean or Manhattan) to the new observation being predicted and then uses the average response value (regression) or the most common class (classification) of those K observations as the predicted output.\n\nDecision Tree\n\nA decision support tool that uses a tree-like model of decisions and their possible consequences. Decision trees comprised three main parts: decision nodes (indicating a split), chance nodes (denoting probability), and end nodes (displaying outcomes). Nodes formed recursively using binary partitions by asking simple yes-or-no questions about each feature (e.g. is “acousticness” < 0.4).\n\nBagged Decision Trees (Bagging)\n\nBootstrap aggregating (bagging) prediction models is a general method for fitting multiple versions of a prediction model (such as decision trees) and then combining them into an aggregated prediction. This helps prevent overfitting and reduces the variability in the model; however, when there are dominant predictors, many of the decision trees end up looking very similar (i.e. are highly correlated), which reduces the impact of bagging. \n\nRandom Forests\n\nRandom forests are a modification of bagged decision trees that build a large collection of de-correlated trees to further improve predictive performance. The trees are decorrelated by adding random selection to which predictors are included in individual trees."
  },
  {
    "objectID": "posts/2023-03-13-spotify-ML-blog/index.html#comparing-model-performance",
    "href": "posts/2023-03-13-spotify-ML-blog/index.html#comparing-model-performance",
    "title": "Whose Song is it Anyway?",
    "section": "Comparing model performance",
    "text": "Comparing model performance\nWhen analyzing the performance of a classification model, it’s important to take class imbalance into consideration. On this occasion, 68% of the songs used in the model belong to my Spotify library, so a model that simply predicts “Lewis” as the listener will be correct roughly 68% of the time. Thus, for a model to be deemed at all successful, it would need to obtain an accuracy score above 68%. \nAlong with accuracy, the area under the ROC curve (receiver operating characteristic curve) is commonly used to measure the performance of a classification model. The curve plots the true positive rate on the y axis and the false positive rate on the x axis, and shows the performance of the model at all classification thresholds. A high performing model will have an ROC AUC (area under the curve) score close to 1, indicating that the model is able to detect true positive results while keeping the false positive rate low. \nThe below table and chart demonstrates how well the various models I used performed in terms of accuracy and ROC AUC scores. \n\n\nCode\n#cleaning model metrics to create a table with each model's performance metrics\n\nforest_results_table <- final_forest_fit %>% collect_metrics() %>%\n  dplyr::select(.metric, .estimate) %>%\n  mutate(method = \"Random Forest\")\n\nbagging_results_table <- final_bag_fit %>% collect_metrics() %>%\n  dplyr::select(.metric, .estimate) %>%\n  mutate(method = \"Bagging\")\n\ndecision_tree_results_table <- final_tree_fit %>% collect_metrics() %>%\n  dplyr::select(.metric, .estimate) %>%\n  mutate(method = \"Decision Tree\")\n\nknn_results_table <- final_fit %>% collect_metrics() %>%\n  dplyr::select(.metric, .estimate) %>%\n  mutate(method = \"KNN\")\n\nmajority_class_results_table <- data.frame (.metric  = c(\"accuracy\", \"roc_auc\"),\n                  .estimate = c(nrow(lewis_liked_tracks) / (nrow(lewis_liked_tracks) + nrow(elke_liked_tracks)), .5),\n                  method = c(\"Dummy Classifier\", \"Dummy Classifier\")\n                  )\n\nfull_results <- bind_rows(majority_class_results_table,\n                          knn_results_table,\n                          decision_tree_results_table,\n                          bagging_results_table,\n                          forest_results_table) %>%\n  dplyr::select(method, .estimate, .metric)\n\n#table of results\nfull_results %>%\n  kbl(caption = \"Table 5. Classification Methods and their Respective Accuracies\") %>%\n  kable_classic(full_width = T, html_font = \"Cambria\")\n\n\n\n\nTable 5. Classification Methods and their Respective Accuracies\n \n  \n    method \n    .estimate \n    .metric \n  \n \n\n  \n    Dummy Classifier \n    0.6774194 \n    accuracy \n  \n  \n    Dummy Classifier \n    0.5000000 \n    roc_auc \n  \n  \n    KNN \n    0.7662651 \n    accuracy \n  \n  \n    KNN \n    0.8254070 \n    roc_auc \n  \n  \n    Decision Tree \n    0.7180723 \n    accuracy \n  \n  \n    Decision Tree \n    0.7515946 \n    roc_auc \n  \n  \n    Bagging \n    0.7518072 \n    accuracy \n  \n  \n    Bagging \n    0.7981592 \n    roc_auc \n  \n  \n    Random Forest \n    0.7638554 \n    accuracy \n  \n  \n    Random Forest \n    0.8089996 \n    roc_auc \n  \n\n\n\n\n\n\n\nCode\n#accuracy plot\nggplot(data = full_results, aes(x = method, y = .estimate, fill = .metric)) +\n  geom_bar(stat='identity', position='dodge') +\n  theme_minimal() +\n  scale_fill_manual(values=c(\"#9fd182\", \"#7798c9\")) +\n  labs(y = \"Accuracy Estimate\",\n       x = \"Classification Method\",\n       fill = \"Accuracy Metric\",\n       title = \"Random Forest Classification Performed the Best Across Both Accuracy Metrics\")\n\n\n\n\n\nThankfully, all of my machine learning models performed better than a dummy classifier (one that always predicts that a song is from my library).\nAcross both performance metrics, the simple decision tree performed the worst out of the models. I believe this is likely due to the class imbalance in the data. Decision trees are “greedy” learners, which means that they don’t think ahead when deciding how to split at any given node.  When one class has low representation, observations can get lost in the majority class nodes and the prediction of the minority class will be even less likely than it should. Essentially, nodes for this decision tree were more likely to contain my songs because my music library is twice as large. For instances where Elke and I listen to similar music, the fact that I have more songs could influence the prediction of the model.\nK-nearest neighbors is also sensitive to class imbalances, as predictions are based solely on the number of similar observations; however, my KNN model outperformed the decision tree model quite substantially with a higher accuracy and much higher ROC AUC. \nThe two ensemble algorithms, bagged decision trees and random forests, performed best on my test data. Random forests outperformed bagging across both accuracy and ROC AUC, making it the best performing model. Due to the class imbalance, it’s likely that many decision trees in the bagging ensemble were highly correlated, so utilizing the random forest approach to decorrelate each individual tree likely helped improve the model efficiency. \nWhile all of the models performed better than a dummy classifier, none were truly exceptional in distinguishing Elke’s liked tracks from mine. When looking at the predictions for each model, I was able to see the percent confidence rate for each prediction—it looked like the models were highly confident when predicting some songs (mainly rap, hip-hop and R&B) from my library, but were much less confident when predicting the alternative/indie music that both Elke and I enjoy."
  },
  {
    "objectID": "posts/2023-03-13-spotify-ML-blog/index.html#final-thoughts",
    "href": "posts/2023-03-13-spotify-ML-blog/index.html#final-thoughts",
    "title": "Whose Song is it Anyway?",
    "section": "Final thoughts",
    "text": "Final thoughts\nOverall, this project was a great opportunity to explore the world of music data analysis and machine learning. It was cool to see the patterns in our music libraries and to compare our tastes on a statistical level. While the results of our classification models were not perfect, they were still quite accurate, and they suggest that there are meaningful differences in the types of songs that Elke and I tend to listen to.\nIf you’re interested in conducting a similar analysis in R, I highly recommend checking out my github repository for this assignment, exploring the Spotify API, and experimenting with the various features and methods available. It’s a great way to gain insight into your own music tastes and to see how they compare to those of your friends and peers."
  },
  {
    "objectID": "posts/2022-12-03-asthma-blog/index.html",
    "href": "posts/2022-12-03-asthma-blog/index.html",
    "title": "Modeling Asthma Hospitalizations",
    "section": "",
    "text": "Asthma is a chronic disease that affects the respiratory system. During an asthma attack, an individual’s airways constrict and swell, causing a combination of coughing, wheezing, shortness of breath, and chest tightness [1]. For some individuals, symptoms can be quite mild and handled easily with preventative medication. For others, asthma can be a life threatening disease that severely reduces their quality of life. \nGlobally, asthma affected roughly 262 million people and caused over 450,000 deaths in 2019 [2]. While not quite as severe in the United States, it’s estimated that 25 million Americans have asthma and over 4,000 die from asthma each year. It is important to note that asthma does not affect all groups equally: statistics provided by the CDC demonstrate that asthma is more common among women, individuals living in poverty, and Black and indigenous people [3]. \nAs someone with asthma, I’m generally pretty cognizant of things that trigger my asthma flare ups. For example, I’ve noticed that I am particularly sensitive to poor air quality. In 2020, I was living in Minnesota during a particularly severe period of Canadian wildfires—the smoke from these fires affected both the air quality and my severity of asthma. During this time, I frequently looked at a variety of weather apps to check the air quality before doing any activity that required me to be outside for an extended period of time. \nIt’s fairly intuitive that air quality would have an impact on asthma, an upper respiratory disease, and this idea is supported by research [4]. However, it is not as clear just how strong this impact is, as well as what other factors are at play in determining asthma attack prevalence. In this study, I will explore how a variety of factors affect the rate of hospitalizations caused by asthma for each county in California. I will also test a hypothesis that there are more asthma related hospitalizations in years when more acres of land burned due to wildfire. As drought continues in California and wildfires seem likely to worsen, it’s prudent that we examine how air quality and wildfire smoke impact asthma."
  },
  {
    "objectID": "posts/2022-12-03-asthma-blog/index.html#data",
    "href": "posts/2022-12-03-asthma-blog/index.html#data",
    "title": "Modeling Asthma Hospitalizations",
    "section": "Data",
    "text": "Data\nIn order to answer the above questions, I obtained data from a variety of sources.\n\nCounty level Air Quality Index (AQI) data was retrieved from the EPA [5].\nBi-yearly asthma prevalence [6] and yearly hospitalization counts [7] were obtained from the California Health and Human Services (CHHS) Open Data Portal .\nA shapefile containing county level geographic information was obtained from the California Open Data Portal [8]. \nSmoke data, measured in daily PM 2.5 estimates, was retrieved from the Environmental Change and Human Outcomes (ECHO) lab at Stanford [9]. \nCounty level demographic information was collected by the United States Census Bureau [10]."
  },
  {
    "objectID": "posts/2022-12-03-asthma-blog/index.html#methods",
    "href": "posts/2022-12-03-asthma-blog/index.html#methods",
    "title": "Modeling Asthma Hospitalizations",
    "section": "Methods",
    "text": "Methods\nOnce I read in each of the datasets, I cleaned them to follow the ‘Tidy Data’ format, selected desired columns and rows, and joined the datasets [11]. New columns, such as the mean smoke (measured in PM 2.5) per year for each county were created. High levels of PM 2.5, fine particles typically smaller than 2.5 µm in diameter, are a great concern to public health because PM 2.5 can travel deeper into the lungs than larger particulate matter. I also adjusted the maximum AQI values to not exceed 500—the EPA’s highest level of healthy concern, “Hazardous,” ranges from index values of 301–500 so any value above that is functionally equivalent. \nAll analyses, specifically an Ordinary Linear Regression (OLS) model and a Welch t-test (α < 0.05) were conducted in Rstudio.\nFor the outcome variable in the regression, I wanted to focus on hospitalizations due to asthma. Hospitalizations, used as a proxy for measuring the severity of asthma attacks, seem particularly sensitive to climate related triggers that could vary over the years for which my data was collected. Since hospitalizations per county vary significantly with the population of the county, I normalized the variable by calculating the hospitalization rate per 100,000 people. \nBefore jumping into modeling, each variable of interest was examined to ensure that it adhered to the assumptions required for OLS. The max AQI and population density explanatory variables were log transformed to normalize outliers and linearize the distribution, and I took the square root of the mean smoke variable for a similar effect. For each of these variables, there was still some minor heteroskedasticity shown in residual plots. There were also more extreme positive residuals than negative residuals; however, the relationships all appeared mostly linear and histograms of the residuals were fairly normal, so I concluded that proceeding with caution would be acceptable. \nFor the Welch t-test, I wanted to examine whether there is a difference in the hospitalization rate during more severe wildfire years. To do this, I compared the mean hospitalization rate in 2016 (669,534 acres burnt due to wildfire) and 2018 (1,975,086 acres burnt due to wildfire)."
  },
  {
    "objectID": "posts/2022-12-03-asthma-blog/index.html#results-discussion",
    "href": "posts/2022-12-03-asthma-blog/index.html#results-discussion",
    "title": "Modeling Asthma Hospitalizations",
    "section": "Results + Discussion",
    "text": "Results + Discussion\nTo determine the magnitude and significance at which various explanatory variables relate to hospitalizations, I conducted an OLS multiple regression. \nFor model selection, I utilized two different approaches. First, I completed the model selection manually by choosing variables/interactions that I thought could be meaningful and then eventually removing ones that were insignificant and didn’t have as strong of a theoretical rationale for why they should be included in the model. Then, I included a backward stepwise model selection process by using the `stepAIC()` function in the MASS package—this allowed me to automate the process of selecting which variables should be included to optimize the model. With backwards steps, we start the stepwise process with all predictors and remove the least statistically significant one until we find the model with the lowest AIC (a measure of goodness of fit) value [12]. Both approaches led me to the same predictor variables of interest: log(max AQI), sqrt(mean PM from smoke), log(population density), and median income. \n\n\nCode\n#CREATING A MAP OF THE HOSPITALIZATION RATE\nhosp_ggplot <- ggplot() +\n  geom_sf(data = hosp_year_summaries, \n          aes(fill = mean_hosp), col = 'black', size = 0.5) +  #fill color is the mean hospitalization rate from asthma, borders are black\n  labs(title = \"Hospitalization Rate from Asthma between \\n 2015–2019 for California Counties\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(), #creating a blank background\n        axis.title.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        axis.text = element_blank(),\n        legend.direction = \"vertical\",  #specifying legend characteristics\n        legend.position = c(1, 0.75),\n        legend.key.height = unit(0.3, \"cm\"),\n        legend.key.width = unit(0.3, \"cm\")) +\n  scale_fill_stepsn(colors = c(\"#fff0d6\",  #specifying the color scheme \n                               \"#ffffb2\", \n                               \"#fecc5c\", \n                               \"#fd8d3c\", \n                               \"#f03b20\", \n                               \"#bd0026\"),\n                    na.value = \"grey50\",\n                    guide = guide_colorbar(title = \"Hospitalization Rate \\n (per 100,000)\")) #specifying the legend title \n\n\n#I then create 4 more plots for each of my explanatory variables. To see this code, checkout my full repository. \n\n\n#SAVING THE IMAGE AND READING IT BACK INTO R\nggsave(filename = \"hosp_map.png\", plot=hosp_ggplot, \n       width=6, height=4, units = \"in\") #save the plot as a png\n\nhosp_map_png <- image_read(\"hosp_map.png\")  #read the png into R\n\n\n\n#MAKING THE ANIMATION\ngif_images <- c(hosp_map_png, income_map_png, density_map_png, aqi_map_png, smoke_map_png)  #Creating a vector of images\n\nasthma_animation <- image_animate(image_scale(gif_images, \"400x400\"), fps = 0.5, dispose = \"previous\") #creating the animation\n\nimage_write(asthma_animation, \"asthma_vars.gif\") \n#if you want to save your gif to your computer, image_write() allows you to do so!\n\nasthma_animation\n\n\n\n\n\nThrough spatial visualization of the data, we can get an initial sense of how a couple of the model’s explanatory variables correlate with the county level hospitalization rate. Southern California and the Central Valley appear to have higher mean AQI values and a higher hospitalization rate. Fine particulate matter from wildfires, meanwhile, appears to have an opposite effect with higher values in Northern California as well as the northern section of the Central Valley. Looking at the population density and median income maps, it is harder to see how they correlate with the hospitalization rate. Below, we examine how each predictor relates to the hospitalization rate in more detail.\n\n\nCode\nasthma_mod <- lm(hosp_per_100k ~ log(max_aqi) + sqrt(mean_smokePM) + log(pop_density_sq_m) + median_income_in_thousands, data = hosp_full)\n\nasthma_hosp_table <- tab_model(asthma_mod,\n                               pred.labels = c(\"Intercept\", \n                                               \"Log Max AQI\", \n                                               \"SQRT Mean Smoke\", \n                                               \"Log Pop Density\", \n                                               \"Median Income (in $1000s)\"),\n                               dv.labels = c(\"Hospitalization Rate (per 100k)\"),\n                               string.ci = \"Conf. Int (95%)\",\n                               string.p = \"P-value\",\n                               title = \"Table 1. Linear Model Results\",\n                               digits = 3)\n\nasthma_hosp_table\n\n\n\n\nTable 1. Linear Model Results\n\n \nHospitalization Rate (per 100k)\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\nIntercept\n0.362\n-23.806 – 24.530\n0.976\n\n\nLog Max AQI\n11.037\n6.467 – 15.608\n<0.001\n\n\nSQRT Mean Smoke\n-3.422\n-5.544 – -1.300\n0.002\n\n\nLog Pop Density\n4.293\n2.839 – 5.747\n<0.001\n\n\nMedian Income (in $1000s)\n-0.408\n-0.535 – -0.281\n<0.001\n\n\nObservations\n249\n\n\nR2 / R2 adjusted\n0.250 / 0.238\n\n\n\n\n\n\nResults from the model indicate that air quality (measured through AQI and particulate matter from smoke), population density, and median income are all statistically significant predictors of the hospitalization rate. \nBefore diving into each predictor variable, it’s important to emphasize the difference between AQI measurements and PM 2.5. The AQI is typically calculated using hourly measurements of 5 pollutants: fine particles (PM 2.5 and PM 10), ground-level ozone, sulfur dioxide, nitrogen dioxide, and carbon monoxide. This means that smoke from wildfires, measured in PM 2.5, is one of the factors that impacts the AQI. By including smoke from wildfires as a separate variable, I am able to examine that separately. \nStarting with the air quality index, there is a positive trend between the log of the maximum AQI and the hospitalization rate (\\(p < 0.001\\), \\(\\beta = 11.037\\)) . Intuitively, this makes sense, as it seems more likely that severe asthma flare-ups could result from dangerous air quality values. \nPrevious studies have indicated that particular matter from wildfires can actually be more harmful for individuals with asthma than from other sources, so it was surprising that the square root of the PM 2.5 caused by wildfire smoke was negatively associated with the hospitalizations (\\(p = 0.002\\), \\(\\beta = -3.422\\)) [13]. One possible explanation for this result is that individuals with asthma were much less likely to go outside during times when wildfire smoke was particularly severe. Compared to the other measures of air quality, wildfire smoke is much more noticeable, so individuals with asthma who have the ability to avoid interacting with the smoke would likely choose to do so. \nThere was a positive trend between the log of the population density per square mile and the county level hospitalization rate (\\(p < 0.001\\), \\(\\beta = 4.293\\)). It makes sense that asthma may be worse in urban areas due to increased pollution, but this \\(\\beta\\) value is the slope for the population density while holding the other variables constant. It’s possible that there are other factors, such as different forms of pollution, increased proximity to hospitals, or other demographic information that could be influencing the effect of population density. Median income, unsurprisingly, had a negative relationship with the hospitalization rate (\\(p < 0.001\\), \\(\\beta = -0.408\\)). Higher income individuals are more likely to be able to afford asthma prevention and likely have access to cleaner indoor air. \nOverall, the model’s predictive power is not too strong. With an \\(R^2\\) of 0.250, just 25% of variability in the per county hospitalization rate can be explained using the explanatory variables. To further test the model’s predictive power, I used it to predict hospitalization rates for my existing data and then compared the predictions to the true observed hospitalization rates in the graph below. The graph demonstrates that the predictions trend in the right direction but vary a fair amount in their accuracy, especially for observations when the true hospitalization rate was above 60 people per 100,000.\n\n\nCode\nasthma_mod <- lm(hosp_per_100k ~ log(max_aqi) + sqrt(mean_smokePM) + log(pop_density_sq_m) + median_income_in_thousands, data = hosp_full) #the final model\n\npredictions <- augment(asthma_mod) #create predictions \n\n#plot the hospitalization rate predictions compared to the true observed value\nprediction_plot <- ggplot(data = predictions, mapping = aes(x = .fitted, y = hosp_per_100k)) +\n  geom_point() +\n  geom_abline(slope = 1, color = \"red\", lwd = 1) + #adds red line that follows what the graph would look like if my predictions perfectly matched the observed values.\n  xlim(20, 70) +\n  annotate(\"text\",\n           x = 67, \n           y = 83, \n           label = \"Line of perfect \\n predictions\",\n           color = \"red\",\n           size = 3.5) +\n  labs(x = \"Predicted Hospitalizations (per 100k)\",\n       y = \"True Hospitalizations (per 100k)\",\n       title = \"Comparing our model predictions to the true value of hospitalizations\",\n       subtitle = \"The model trends in the right direction, but tends to over predict low hospitalization rates and underpredict low hospitalization rates. A perfect model would follow the line in red.\") +\n  theme_classic()\n\nprediction_plot\n\n\n\n\n\nIn order to further test the effect of wildfires on asthma, I performed a t-test (alpha < 0.05) to compare the mean hospitalization rate caused by asthma in 2016 and 2018. For context, 2016 was a relatively light year for wildfires, with 669,534 acres burnt; in comparison, almost three times as many acres (1,975,086) burned in 2018 [14]. Seeing if there is a difference in the hospitalization rate between these two years could demonstrate the effect of wildfires on asthma hospitalizations. \nThe null hypothesis: There is no difference in the mean hospitalization rate between 2016 and 2018. \n\n\\(H_0:\\mu_{hosp2016} - \\mu_{hosp2018} = 0\\)\n\nThe alternative hypothesis: This is a difference in the mean hospitalization rate between 2016 and 2018.\n\n\\(H_A:\\mu_{hosp2016} - \\mu_{hosp2018} \\neq 0\\) \n\nMy t-test concluded that there is not enough evidence to reject the null hypothesis (t = -0.18629, p = 0.8526). This indicates that, if there was a true difference in the mean hospitalization rate, we’d expect to find a difference in means as extreme as ours 85% of the time. This aligns with the regression model conducted above, but does not align with previous research that smoke from wildfires increases cases of severe asthma [13]. \nHowever, it is important to note that this result does not mean that the total acreage of wildfires burnt has no effect on asthma hospitalizations. It’s possible that the location of the fire and other factors, such as wind patterns and population density near the fire, are affecting the result. Further research could look into the spatial relationship between fires and where asthma related hospitalizations occur.\n\n\n\nCode\n## Hypothesis test that hospitalizations would be different (likely higher) in 2018 than in 2016 due to increased fire acreage burnt. \n\n#2016: 669,534 burnt\n#2018:  1,975,086 burnt\n\nhosp_2016_2018 <- hosp_full %>%\n  filter(year == 2016 | year == 2018) %>%\n  mutate(year = as.factor(year))\n\nfire_test <- t.test(hosp_per_100k ~ year, data = hosp_2016_2018)\n\nfire_test_table <- tab_model(fire_test,\n          string.ci = c(\"Conf. Int (95%)\"),\n          string.p = \"P-value\",\n          dv.labels = c(\"Hospitalization Rate\"),\n          pred.labels = \"2016 – 2018\",\n          title = \"Table 2: Hospitalization Rate and Wildfires: Welch Two Sample t-test\")\n\nfire_test_table\n\n\n\n\nTable 2: Hospitalization Rate and Wildfires: Welch Two Sample t-test\n\n \nHospitalization Rate\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\n2016 – 2018\n-0.51\n-5.94 – 4.92\n0.853\n\n\n\n\n\n\n\n\nCode\n#Creating a beeswarm plot with boxplot overlay to show distribution of county level mean hospitalization rates for 2016 and 2018. \n\nbeeswarm_boxplot <- ggplot(data = hosp_2016_2018, mapping = aes(x = year, y = hosp_per_100k)) +\n  geom_beeswarm(color = \"red\", alpha = .5) + \n  geom_boxplot(alpha = 0.3) +\n  theme_classic() +\n  labs(x = \"Year\",\n       y = \"Hospitalizations (per 100k)\",\n       title = \"California County Hospitalization Rate from Asthma in 2016 and 2018\",\n       subtitle = \"The distribution of hospitalization rates appear similar from 2016 and 2018\")\n\n#add in t-test and p value to the plot\nbeeswarm_boxplot_t_test <- beeswarm_boxplot + \n  stat_compare_means(method = \"t.test\", label.x = 1.35, label.y = 65)\n\nbeeswarm_boxplot_t_test"
  },
  {
    "objectID": "posts/2022-12-03-asthma-blog/index.html#conclusions",
    "href": "posts/2022-12-03-asthma-blog/index.html#conclusions",
    "title": "Modeling Asthma Hospitalizations",
    "section": "Conclusions",
    "text": "Conclusions\nWhile this research is an important step in exploring the effect of air quality on hospitalizations from asthma in California counties, there are a number of limitations of this analysis that could be addressed in future research. \nIf possible, further analysis should explore the temporal relation between air quality and hospitalizations from asthma on a more granular scale. I was only able to obtain hospitalization data at an annual scale, which made it harder to see how shorter spikes in poor air quality affected asthma related hospitalizations, if at all. \nIt would also be worth exploring a similar analysis but at the census tract, rather than county level. Counties are sufficiently large that the air quality is likely not uniformly distributed throughout. Furthermore, focusing on census tracts could open up the possibility of exploring income in more detail. Our results indicated that higher income averages were associated with lower hospitalization rates from asthma. It’s possible that this finding would be even more pronounced at the census tract, rather than county level. \nThis analysis also has important implications for environmental justice. As mentioned in the introduction, asthma is a disease that affects humans across the globe but more acutely in developing countries. Looking at the World Health Organization’s data for the death rate per 100,000 due to asthma, Kiribati, New Guinea, and Lesotho have rates of 75.4, 47.6, and 45.3, respectively (compared to the US rate of 0.84) [15]. While much of this inequality can likely be attributed to a difference in access to medicine, it is worth exploring what other factors, including air pollution, could be causing such high rates of severe asthma in certain countries. Partnering with researchers in nations where asthma is particularly severe could help address the disparity in prevalence, hospitalization, and death rates across the globe. \nTo summarize, this research examines a number of factors that relate to hospitalizations from severe asthma. From this analysis, it’s clear that clean air is pertinent to preventing severe asthma flare-ups. Given the potential severity of asthma and its prevalence in the US and across the globe, further research should examine this topic to better understand the primary factors that lead to the most severe cases of asthma.\n\nReferences\n[1] “Asthma - Symptoms and causes,” Mayo Clinic. https://www.mayoclinic.org/diseases-conditions/asthma/symptoms-causes/syc-20369653 (accessed Dec. 03, 2022).\n[2] “Asthma.” https://www.who.int/news-room/fact-sheets/detail/asthma (accessed Dec. 02, 2022).\n[3] “Most Recent National Asthma Data | CDC,” May 26, 2022. https://www.cdc.gov/asthma/most_recent_national_asthma_data.htm (accessed Dec. 02, 2022).\n[4] O. US EPA, “The Links Between Air Pollution and Childhood Asthma,” Oct. 22, 2018. https://www.epa.gov/sciencematters/links-between-air-pollution-and-childhood-asthma (accessed Dec. 04, 2022).\n[5] “AirData website File Download page,” United States Environmental Protection Agency. https://aqs.epa.gov/aqsweb/airdata/download_files.html (accessed Dec. 02, 2022).\n[6] “Asthma Prevalence - California Health and Human Services Open Data Portal,” CHHS Open Data. https://data.chhs.ca.gov/dataset/asthma-prevalence (accessed Dec. 02, 2022).\n[7] “Asthma Hospitalization Rates by County - California Health and Human Services Open Data Portal,” CHHS Open Data. https://data.chhs.ca.gov/dataset/asthma-hospitalization-rates-by-county (accessed Dec. 02, 2022).\n[8] “CA Geographic Boundaries - California Open Data,” California Open Data Portal. https://data.ca.gov/dataset/ca-geographic-boundaries (accessed Dec. 02, 2022).\n[9] “wildfire_smoke — ECHO: Environmental Change and Human Outcomes Lab | Stanford University,” ECHO:  Environmental Change and Human Outcomes Lab    |    Stanford University. https://www.stanfordecholab.com/wildfire_smoke (accessed Dec. 02, 2022).\n[10] R. Whitcomb, J. M. Choi, and B. Guan, “CORGIS Datasets Project,” County Demographics CSV File. https://corgis-edu.github.io/corgis/csv/county_demographics/ (accessed Dec. 02, 2022).\n[11] “Data tidying.” Accessed: Dec. 04, 2022. [Online]. Available: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html\n[12] R. statistics for P. Science, “Choose model variables by AIC in a stepwise algorithm with the MASS package in R,” R Functions and Packages for Political Science Analysis, Oct. 22, 2020. https://rforpoliticalscience.com/2020/10/23/choose-model-variables-by-aic-in-a-stepwise-algorithm-with-the-mass-package-in-r/ (accessed Dec. 04, 2022).\n[13] D. Kiser et al., “Particulate matter and emergency visits for asthma: a time-series study of their association in the presence and absence of wildfire smoke in Reno, Nevada, 2013–2018,” Environ. Health, vol. 19, no. 1, p. 92, Aug. 2020, doi: 10.1186/s12940-020-00646-2.\n[14] “California Wildfires History & Statistics | Frontline Wildfire Defense,” Frontline, Sep. 30, 2020. https://www.frontlinewildfire.com/wildfire-news-and-resources/california-wildfires-history-statistics/ (accessed Dec. 04, 2022).\n[15] “ASTHMA DEATH RATE BY COUNTRY,” World Life Expectancy. https://www.worldlifeexpectancy.com/cause-of-death/asthma/by-country/ (accessed Dec. 05, 2022).\n\n\nSupporting figures\nGithub repository: https://github.com/lewis-r-white/lewis-r-white.github.io\n\n\nCode\n#log max aqi on percent hosp rate\nggplot(data = hosp_full, aes(x = log(max_aqi), y = hosp_per_100k)) +\n  geom_point() +\n  theme_classic() +\n  labs(x = \"Log Max AQI\",\n       y = \"Hospitalization Rate (per 100,000)\",\n       title = \"Checking linearity of log max AQI\")\n\n\n\n\n\nCode\nmax_aqi_mod <- lm(hosp_per_100k ~ log(max_aqi), data = hosp_full)\n\nres_max_aqi <- resid(max_aqi_mod)\n\nplot(fitted(max_aqi_mod), res_max_aqi) +\n  abline(0,0) +\n  title(\"Residual plot for max AQI\") # appears fairly linear, although the residuals in the middle are more extreme than at the ends. Proceed with caution. \n\n\n\n\n\ninteger(0)\n\n\nCode\n# pop density on hosp rate\nggplot(data = hosp_full, aes(x = log(pop_density_sq_m), y = hosp_per_100k)) +\n  geom_point() +\n  theme_classic() +\n  labs(x = \"Log Population Density\",\n       y = \"Hospitalization Rate (per 100,000)\",\n       title = \"Checking linearity of log population density\")\n\n\n\n\n\nCode\npop_density_mod <- lm(hosp_per_100k ~ log(pop_density_sq_m), data = hosp_full)\n\nres_pop_density <- resid(pop_density_mod)\n\nplot(fitted(pop_density_mod), res_pop_density) +\n  abline(0,0) +\n  title(\"Residual plot for log population density\")\n\n\n\n\n\ninteger(0)\n\n\nCode\n## median income on hosp\nggplot(data = hosp_full, aes(x = median_income_in_thousands, y = hosp_per_100k)) +\n  geom_point() +\n  theme_classic() +\n  labs(x = \"Median Income (in 1000s)\",\n       y = \"Hospitalization Rate (per 100,000)\",\n       title = \"Checking linearity of Median Income\")\n\n\n\n\n\nCode\nincome_mod <- lm(hosp_per_100k ~ median_income_in_thousands, data = hosp_full)\n\nres_income <- resid(income_mod)\n\nplot(fitted(income_mod), res_income) +\n  abline(0,0) +\n  title(\"Residual plot for median income shows some light fanning\") # a little bit of fanning ~ proceed with caution\n\n\n\n\n\ninteger(0)\n\n\nCode\n## mean PM 2.5 from fire\nggplot(data = hosp_full, aes(x = sqrt(mean_smokePM), y = hosp_per_100k)) +\n  geom_point() +\n  theme_classic() +\n  labs(x = \"Square Root of Mean Smoke (PM 2.5)\",\n       y = \"Hospitalization Rate (per 100,000)\",\n       title = \"Checking linearity of Mean Smoke\")\n\n\n\n\n\nCode\nsmoke_mod <- lm(hosp_per_100k ~ sqrt(mean_smokePM), data = hosp_full)\n\nres_smoke <- resid(smoke_mod)\n\nplot(fitted(smoke_mod), res_smoke) +\n  abline(0,0) +\n  title(\"Residual plot for mean smoke shows some light fanning\") # a little bit of fanning ~ proceed with caution\n\n\n\n\n\ninteger(0)"
  },
  {
    "objectID": "posts/2023-01-08-ai-ethics-podcast/index.html",
    "href": "posts/2023-01-08-ai-ethics-podcast/index.html",
    "title": "Ethics of AI and Behavioral Nudging for the Environment",
    "section": "",
    "text": "In this podcast, Elke Windschitl and I discuss the ethics of using behavioral nudging tactics and artificial intelligence to help tackle climate issues. Positions were assigned at random. Elke argued in favor of using AI but against using nudging tactics while I argued the opposing positions.\nThis podcast was created as a final project for EDS 242: Ethics and Bias in Environmental Data Science, an ethics course in UCSB’s Master’s of Environmental Data Science curriculum taught by Dena Montague.\nIntro music was created by Bonfire Records and moderation was conducted by Jessica French.\n\n\n\nelkewind · Debating Nudging and AI for Climate\n\n\n\n\nCitationBibTeX citation:@online{white,elkewindschitl2023,\n  author = {Lewis White, Elke Windschitl},\n  title = {Ethics of {AI} and {Behavioral} {Nudging} for the\n    {Environment}},\n  date = {2023-01-08},\n  url = {https://lewis-r-white.github.io/posts/2023-01-08-ai-ethics-podcast/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLewis White, Elke Windschitl. 2023. “Ethics of AI and Behavioral\nNudging for the Environment.” January 8, 2023. https://lewis-r-white.github.io/posts/2023-01-08-ai-ethics-podcast/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lewis White",
    "section": "",
    "text": "I’m Lewis ~ welcome to my website!\nI’m currently pursuing a Master’s Degree in Environmental Data Science from the Bren School of Science & Management at UCSB.\nIn my spare time, you can find me hiking in the beautiful Santa Barbara hills or practicing my dinks on the pickleball court."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Lewis White",
    "section": "Education",
    "text": "Education\nMS in Environmental Data Science (anticipated 2023)\nUniversity of California, Santa Barbara | The Bren School of Environemntal Science and Management\nBA in Statistics and Psychology (2020)\nCarleton College | Northfield, MN"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Lewis White",
    "section": "Experience",
    "text": "Experience\nQualitative Research Advisor | dscout | 07/2020 - 07/2022\nAccount Management Intern | BBDO Worldwide | 06/2019 - 08/2019\nStatistical Consultant | Northfield City Council | 01/2018 - 03/2018"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bio",
    "section": "",
    "text": "After graduating from Carleton College in 2020 with a degree in Statistics and Psychology, Lewis took a position in the UX space as a research advisor at dscout, a software company with a qualitative research platform. For two years, he primarily collaborated on research design with innovative companies such as Nike, Microsoft, and Sonos, and also conducted analysis to help optimize dscout’s platform. While Lewis enjoyed the collaboration and curiosity that existed within the research world, he wanted to apply the breadth of skills acquired from both college and this dscout experience toward a cause he cares deeply about.\nLewis is now pursuing a Master’s Degree in Environmental Data Science from the Bren School of Science & Management at UC Santa Barbara. Growing up with Scottish parents, Lewis spent many summers on the west coast of Scotland where he developed a keen interest in the natural world, spending many hours exploring and understanding life in the tide pools. A watercolor enthusiast, Lewis is drawn toward data visualization because of its ability to communicate important information in an engaging way. Upon completion of this master’s program, Lewis hopes to use data modeling, data visualization, and scientific communication skills to tackle aspects of the many problems that climate change is bringing to our world. He is particularly interested in extreme weather events and the effect of climate change on human health."
  },
  {
    "objectID": "Posts.html",
    "href": "Posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Ethics of AI and Behavioral Nudging for the Environment\n\n\n\nMEDS\n\n\n\nA debate about how AI and nudge theory should or shouldn’t be used to tackle environmental problems.\n\n\n\nLewis White, Elke Windschitl\n\n\nJan 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Asthma Hospitalizations\n\n\n\nMEDS\n\n\nR\n\n\nAsthma\n\n\n\nExploring factors that effect the hospitalization rate due to asthma in California counties.\n\n\n\nLewis White\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Song is it Anyway?\n\n\n\nMEDS\n\n\nR\n\n\nMachine Learning\n\n\n\nEVisualizing my Spotify data and creating a model to determine whether a song comes from my library or my friend Elke’s.\n\n\n\nLewis White\n\n\nMar 12, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]